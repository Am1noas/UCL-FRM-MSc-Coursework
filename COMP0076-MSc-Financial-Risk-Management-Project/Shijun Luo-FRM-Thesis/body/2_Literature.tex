\chapter{Literature Review} \label{Chap2}
\section{Introduction to Survival Analysis Models}
Survival analysis is a fundamental tool in various fields, especially in medical and social sciences, where understanding the time until an event of interest occurs is crucial. One of the seminal texts in this field is "Survival Analysis: A Self-Learning Text" by Kleinbaum et al. (1996) \cite{kleinbaum1996survival}. This second edition, following the format of the first, presents topics in a lecture-book style. Each chapter includes objectives, outlines, key formulae, practice exercises, and tests, providing a comprehensive learning experience.

\subsection{Kaplan-Meier Estimator}
The Kaplan-Meier estimator \cite{kaplan1958nonparametric} revolutionized survival analysis by introducing a non-parametric method to estimate the survival function from lifetime data. Meyer (1988) \cite{meyer1988unemployment} examined the effects of unemployment insurance benefits on unemployment durations. His findings highlighted the impact of benefit levels and duration on individuals' behavior and the probability of exiting unemployment. The Kaplan-Meier method's significance in medical research is underscored by Stalpers et al. (2018) \cite{stalpers2018edward}, detailing its historical importance in patient survival analysis. Jager et al. (2008) \cite{jager2008analysis} provided insights into the method's application in renal function recovery studies, emphasizing its utility in comparing survival among different groups.

Additionally, Kaplan-Meier estimation finds application in Ayurveda research, as demonstrated by Goel et al. (2010) \cite{goel2010understanding}. Their study illustrates its effectiveness in comparing the survival of subjects under different treatments, even when faced with challenges like censored observations.

Dabrowska et al. (1988) \cite{dabrowska1988kaplan} extended the Kaplan-Meier estimator to the general multivariate case, exploring the estimation of bivariate survival functions from censored data. Miller (1983) \cite{miller1983price} delved into the asymptotic efficiency of the Kaplan-Meier estimator concerning parametric survival function estimators under a random-censoring model. Zhou et al. (1991) \cite{zhou1991some} extended the analysis of Kaplan-Meier estimator to cases where survival and censoring times are not identically and independently distributed (i.i.d.). Furthermore, Efron (1988) \cite{efron1988logistic} discussed the use of standard logistic regression techniques for estimating hazard rates and survival curves from censored data, demonstrating its connection with Kaplan-Meier curves.

Stel et al. (2011) \cite{stel2011survival} provided a detailed explanation of the Kaplan-Meier method, emphasizing its application in analyzing 'time-to-event' data, including all-cause mortality and other specific outcomes.

Peterson (1977) \cite{peterson1977expressing} presented a novel expression of the Kaplan-Meier estimator for censored data, highlighting its strong consistency. Statistical software like SPSS Statistics was utilized to facilitate Kaplan-Meier analysis \cite{statistics2015kaplan}.

Additionally, researchers have explored the performance of the Kaplan-Meier estimator under various conditions. Meier et al. (2004) \cite{meier2004price} reevaluated its efficiency concerning parametric estimators, emphasizing its reliability, especially for estimating the mean or restricted mean lifetime. Akritas et al. (1986) \cite{akritas1986bootstrapping} studied bootstrapping techniques for estimating the survival distribution function under random censoring, enhancing the estimator's applicability.

\subsection{Nelson-Aalen Estimator}
The Nelson-Aalen estimator is an alternative to Kaplan-Meier for estimating the cumulative hazard function. Datta and Sen (2001) \cite{datta2001validity} demonstrated its applicability in estimating integrated transition hazards and stage occupation probabilities in non-Markov multistage models. Borgan et al. (2014) \cite{borgan2014n} discussed its use in nonparametric estimation of cumulative hazard rates in the presence of right-censored and/or left-truncated survival data. De Uña-Álvarez et al. (2004) \cite{de2004nelson} proposed a new inference model for data sampled in a biased manner, introducing empirical distribution methods that generalize the Kaplan-Meier product-limit approach.

Bluhmki et al. (2018) \cite{bluhmki2018wild} proposed a wild bootstrap resampling technique for nonparametric inference on transition probabilities in a time-inhomogeneous Markov multistate model, utilizing the Nelson-Aalen estimator. Brahimi and Bouzebda (2015) \cite{brahimi2015nelson} utilized Nelson-Aalen's estimator for tail product-limit process in randomly right-censored heavy-tailed data, introducing a reduced-bias estimator for the extreme value index.

Comparing Kaplan-Meier and Nelson-Aalen Estimators
Colosimo et al. (2002) \cite{colosimo2002empirical} conducted extensive Monte Carlo simulations to compare the performance of Kaplan-Meier and Nelson-Aalen estimators. Their study revealed subtle differences favoring the Nelson-Aalen estimator in survival fraction estimation. Klein and Moeschberger (1991) \cite{klein1991small} examined the exact moments of various estimators' variance for both Kaplan-Meier and Nelson-Aalen, providing valuable insights into their small-sample properties.

Furthermore, modifications and enhancements to these estimators have been explored. Cao and Yin (2005) \cite{cao2005presmoothed} proposed modifications to Kaplan-Meier and Nelson-Aalen estimators using nonparametric regression, showing improvements in asymptotic mean squared error. Luo and Ying (2011) \cite{luo2011conditional} introduced kernel-assisted conditional Nelson-Aalen and Kaplan-Meier estimators, extending their asymptotic properties and order of remainder terms.

Hu et al. (2020) \cite{hu2020modified} introduced modified versions of Kaplan-Meier and Nelson-Aalen estimators incorporating geographical weights for localized survival analysis. Njamen and Njatchou (2014) \cite{njamen2014nelson} adapted stochastic processes to Nelson-Aalen and Kaplan-Meier estimators in competing risks scenarios.

Additionally, various studies have focused on the convergence properties and asymptotic normality of these estimators in different contexts, ensuring their reliability under diverse conditions \cite{zhousemiparametric}\cite{anevski2017functional}\cite{njomen2019asymptotic}.

\section{RFQ Estimation Methods}
The Request for Quote (RFQ) process in business-to-business supply chains plays a crucial role in supplier decision-making. Several methods have been developed to enhance the decision-making process and improve outcomes for suppliers.

Goodwin et al. (2002) introduced an agent-based decision support system aimed at assisting suppliers in responding to RFQs. Their prototype system provided suggested ways of fulfilling requests and presented alternatives that illustrated tradeoffs in quality, cost, and timelines. The system allowed decision-makers to consider alternatives that reduced costs and improved customer value. The authors implemented the system in Java and further enhanced it by incorporating probabilistic reasoning techniques, enabling the creation of conditional plans that maximize expected utility, considering the risk preferences of the decision-maker. Additionally, they explored the use of data mining techniques to infer customer preferences and estimate the probability of winning an order with a given quote \cite{goodwin2002decision}.

Xue et al. (2016) addressed the pricing strategies for personalized product bundles in response to RFQs. In this context, sellers provide various products for customers to construct personalized bundles, and determining an optimal price for such configurations is challenging due to the numerous possible combinations and correlations among products. The authors proposed a novel top-down and bottom-up approach. In the top-down step, they decomposed the bundle into components, calibrated value scores for each component, and then aggregated the components back to the bundle. In the bottom-up step, they defined essential features of the bundle and segmented RFQs based on these features and customer attributes. Using historical sales data, they estimated utility functions for each segment and derived optimal prices for incoming RFQs. Empirical data from a major information technology service provider demonstrated that this approach significantly improved pricing effectiveness \cite{xue2016pricing}.

Lo et al. (2002) developed and estimated an econometric model of limit-order execution times using survival analysis and actual limit-order data. Their model considered time-to-first-fill and time-to-completion for both buy and sell limit orders, incorporating explanatory variables such as limit price, bid/offer spread, and market volatility. The study revealed that execution times were highly sensitive to the limit price but not to the limit size. The authors highlighted the importance of using actual limit-order data rather than hypothetical executions derived from theoretical or empirical sources for accurate modeling \cite{lo2002econometric}.

The scarcity of research papers exploring the utilization of generative analytical models for price prediction poses a significant challenge. However, this gap in the literature serves as a strong motivation for my research in this specific domain. Recognizing the untapped potential of generative analytical models in predicting prices, I am inspired to delve into uncharted territories, hoping to contribute valuable insights to this underexplored area of study.

\section{Backtesting Approaches}
Backtesting, a crucial component in evaluating the performance of prediction models, has been a subject of extensive research. Two notable approaches are discussed here.

Harrell et al. (1982) introduced the Harrell concordance index, which focuses on evaluating the amount of information a medical test provides about individual patients, especially those with chronic diseases. The method emphasizes the prognostic information a test offers and advocates combining information from various sources, such as history, physical examination, and routine procedures, when assessing the utility of a new test. As an illustration, the study applied this method to evaluate the treadmill exercise test's prognosis in patients with suspected coronary artery disease, demonstrating its effectiveness in assessing prognostic information \cite{harrell1982evaluating}.

Cui et al. (2019) utilized the Random Survival Forest (RSF) method as a cross-validation approach to test the performance of survival analysis models. Their study investigated statistical properties, including consistency, splitting rules, and the influence of the censoring mechanism of RSF and survival trees. By employing RSF, the authors aimed to enhance the robustness and reliability of survival analysis models, emphasizing the importance of rigorous validation methods in survival analysis research \cite{cui2019consistency}.

LeBaron (1998) emphasized the limitations of static data splitting and advocated for bootstrap resampling to assess model uncertainty. The study underscored the significance of considering variability across different data splits and avoiding over-interpretation of specific models \cite{lebaron1998bootstrap}.

In summary, the literature extensively explores the applications, enhancements, and comparative analyses of Kaplan-Meier and Nelson-Aalen estimators, making significant contributions to the field of survival analysis. Also, these backtesting approaches stress the need for comprehensive evaluation methods and robust validation techniques in predictive modelling, enhancing the reliability and applicability of models across diverse fields. This chapter provides a comprehensive overview of these efforts, laying the foundation for the subsequent research in this area.
